# -*- coding: utf-8 -*-
"""
Created on Sat Jan 30 12:46:02 2021

@author: wzhangcd
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import SmoothL1Loss
from torch.nn.parameter import Parameter
from torch.nn.modules.module import Module

import math
import numpy as np
import pandas as pd

import sys

sys.path.append('./lib/')
from lib.pkl_process import *
from lib.utils import load_graphdata_channel_my, compute_val_loss_sttn

from time import time
import shutil
import argparse
import configparser
from tensorboardX import SummaryWriter
import os

from lib.utils import *


from FS_ST_encoder import FS_ST_Transformer

# %%

os.environ['CUDA_VISIBLE_DEVICES'] = "0"

if __name__ == '__main__':

    #params_path = 'Experiment/PEMS04_embed_size64_2loss'  # Path for saving network parameters
    #filename = 'PEMS04/pems04_r1_d0_w0_2loss_crossattention_1.npz'  # Data generated by prepareData.py
    #adj_filename = './PEMS04/distance.csv'  # path for adjacency_matrix

    params_path = 'Experiment/PEMS08_embed_size64_2loss'
    filename = 'PEMS08/pems08_r1_d0_w0_2loss_crossattention_1.npz'
    adj_filename = './PEMS08/distance.csv'

    print('params_path:', params_path)

    num_of_hours, num_of_days, num_of_weeks = 1, 0, 0  # The same setting as prepareData.py

    # Training Hyparameter
    # device = "cuda:0" if torch.cuda.is_available() else "cpu"
    # DEVICE = device
    device = torch.device('cuda:0')
    # batch_size = 4 # pems04
    batch_size = 4 # pems08
    learning_rate = 0.01
    epochs = 80

    # Generate Data Loader
    # load_graphdata_channel_my --- flow data loader & speed data loader
    train_loader, train_target_tensor, val_loader, val_target_tensor, test_loader, test_target_tensor, _mean, _std = load_graphdata_channel_my(
        filename, num_of_hours, num_of_days, num_of_weeks, device, batch_size)

    # Adjacency Matrix Import
    # adj_mx = pd.read_csv('./PEMSD7/W_25.csv', header=None)
    # adj_mx = np.array(adj_mx)

    #num_of_vertices = 307  # pems04
    num_of_vertices = 170  # pems08

    adj_mx, distance_mx = get_adjacency_matrix(adj_filename, num_of_vertices, id_filename=None)
    A = adj_mx
    A = torch.Tensor(A)

    ### Training Hyparameter
    in_channels = 1  # Channels of input
    embed_size = 256  # Dimension of hidden embedding features
    time_num = 288
    num_layers = 3  # Number of ST Block
    T_dim = 12  # Input length, should be the same as prepareData.py
    output_T_dim = 12  # Output Expected length
    heads = 8  # Number of Heads in MultiHeadAttention
    cheb_K = 3  # Order for Chebyshev Polynomials (Eq 2)
    forward_expansion = 4  # Dimension of Feed Forward Network: embed_size --> embed_size * forward_expansion --> embed_size
    dropout = 0

    # 带 flow_speed cross-attention 模块的 sttn
    net = FS_ST_Transformer(
        A,
        in_channels,
        embed_size,
        time_num,
        num_layers,
        T_dim,
        output_T_dim,
        heads,
        cheb_K,
        forward_expansion,
        num_of_vertices,
        dropout)

    net.to(device)

    # Training Process
    # Load the parameter we have already learnt if start_epoch does not equal to 0
    start_epoch = 0
    if (start_epoch == 0) and (not os.path.exists(params_path)):
        os.makedirs(params_path)
        print('create params directory %s' % (params_path))
    elif (start_epoch == 0) and (os.path.exists(params_path)):
        shutil.rmtree(params_path)
        os.makedirs(params_path)
        print('delete the old one and create params directory %s' % (params_path))
    elif (start_epoch > 0) and (os.path.exists(params_path)):
        print('train from params directory %s' % (params_path))
    else:
        raise SystemExit('Wrong type of model!')

    # Loss Function Setting
    criterion = SmoothL1Loss().to(device)
    optimizer = torch.optim.AdamW(net.parameters(), lr=0.01)
    # optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=1e-5)  # 使用AdamW优化器，将weight_decay参数添加到这里
    # optimizer = torch.optim.AdamW(net.parameters(), lr=0.001)  # 使用AdamW优化器，将weight_decay参数添加到这里


    # Training Log Set and Print Network, Optimizer
    sw = SummaryWriter(logdir=params_path, flush_secs=5)
    print(net)
    print('Optimizer\'s state_dict:')
    for var_name in optimizer.state_dict():
        print(var_name, '\t', optimizer.state_dict()[var_name])

    global_step = 0
    best_epoch = 0
    best_val_loss = np.inf
    start_time = time()

    # Load parameters from files
    if start_epoch > 0:
        params_filename = os.path.join(params_path, 'epoch_%s.params' % start_epoch)
        net.load_state_dict(torch.load(params_filename))
        print('start epoch:', start_epoch)
        print('load weight from: ', params_filename)

    # train model
    for epoch in range(start_epoch, epochs):
        # Parameter Saving
        params_filename = os.path.join(params_path, 'epoch_%s.params' % epoch)
        # Evaluate on Validation Set
        val_loss = compute_val_loss_fsst_encode_only(net, val_loader, criterion, sw, epoch)
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch
            torch.save(net.state_dict(), params_filename)
            print('save parameters to file: %s' % params_filename)

        net.train()  # ensure dropout layers are in train mode
        for batch_index, batch_data in enumerate(train_loader):

            encoder_inputs, _, labels = batch_data

            flow_labels = labels[:, :, 0, :]  # b,n,t
            speed_labels = labels[:, :, -1, :]  # b,n,t

            optimizer.zero_grad()



            flow_outputs, speed_outputs = net(encoder_inputs.permute(0, 2, 1, 3))

            loss = criterion(flow_outputs, flow_labels) + criterion(speed_outputs, speed_labels)

            loss.backward()
            optimizer.step()

            training_loss = loss.item()
            global_step += 1
            sw.add_scalar('training_loss', training_loss, global_step)
            if global_step % 1000 == 0:
                print('global step: %s, training loss: %.2f, time: %.2fs' % (
                global_step, training_loss, time() - start_time))

        # scheduler.step()

    print('best epoch:', best_epoch)
